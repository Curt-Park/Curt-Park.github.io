---
layout: post
title: "[전격해부] YOLO"
description: "Paper study of YOLO published in May 2016"
date: 2017-03-26
tags: [object detection]
comments: true
share: true
use_math: true
---

![]({{ site.url }}/images/yolo/Title.JPG "Title"){: .aligncenter}

# Introduction
사람은 어떤 이미지를 봤을때, 이미지 내부에 있는 Object들의 디테일을 한 눈에 파악할 수 있다.
(Object가 무엇인지, 어디에 위치해있는지, 그들은 어떤 관계에 있는지 등) 
적은 의식적 사고의 개입으로도 운전과 같은 복잡한 행위를 할 수 있는 이유도 여기에 있다. 
허나, 근래의 R-CNN과 같은 detection system들은 복잡한 처리과정으로 인해 
이러한 Human visual system을 모방하기에는 부족한 부분들을 보인다. (느린 속도, 최적화의 어려움) 

YOLO(You Only Look Once)는 이미지 내의 bounding box와 class probability를 
single regression problem으로 간주하여, 이미지를 한 번 보는 것으로 object의 종류와 위치를 추측한다.
아래와 같이 single convolutional network를 통해 multiple bounding box에 대한 
class probablility를 계산하는 방식이다. 
![]({{ site.url }}/images/yolo/Figure1.JPG "Figure1"){: .aligncenter} 

기존의 object detection method와 비교했을 때, YOLO가 보여주는 상대적인 장점과 단점은 다음과 같다.

**장점:** 

- 간단한 처리과정으로 속도가 매우 빠르다. 또한 기존의 다른 real-time detection system들과 비교할 때,2배 정도 높은 mAP를 보인다.
- Image 전체를 한 번에 바라보는 방식으로 class에 대한 맥락적 이해도가 높다. 이로인해 낮은 backgound error(False-Positive)를 보인다.
- Object에 대한 좀 더 일반화된 특징을 학습한다. 가령 natural image로 학습하고 이를 artwork에 테스트 했을때, 다른 Detection System들에 비해 훨씬 높은 성능을 보여준다.



**단점:**

- 상대적으로 낮은 정확도 (특히, 작은 object에 대해)



차후 실험결과를 통해 위와 같은 tradeoff들에 대해 좀 더 자세히 살펴보도록 하겠다.

# Unified Detection
Overview: 
![]({{ site.url }}/images/yolo/Figure2.JPG "Figure2"){: .aligncenter}

1. Input image를 S X S grid로 나눈다.
2. 각각의 grid cell은 B개의 bounding box와 각 bounding box에 대한 confidence score를 갖는다. 
(만약 cell에 object가 존재하지 않는다면 confidence score는 0이 된다.)    
**Confidence Score**: $$Pr(Object) * IOU_{truth pred}$$
3. 각각의 grid cell은 C개의 conditional class probability를 갖는다.    
**Conditional Class Probability**: $$Pr(Class_{i}|Object)$$
4. 각각의 bounding box는 x, y, w, h, confidence로 구성된다.    
(x,y): Bounding box의 중심점을 의미하며, grid cell의 범위에 대한 상대값이 입력된다.    
(w,h): 전체 이미지의 width, height에 대한 상대값이 입력된다.    
    - *예1: 만약 x가 grid cell의 가장 왼쪽에 있다면 x=0, y가 grid cell의 중간에 있다면 y=0.5*    
    - *예2: bbox의 width가 이미지 width의 절반이라면 w=0.5*

Test time에는 conditional class probability와 bounding box의 confidence score를 곱하여 class-specific confidence score를 얻는다. 

$$
\begin{align}
Class Specific Confidence Score &= Conditional Class Probability * Confidence Score \\
&= Pr(Class{i}|Object) * Pr(Object) * IOU{truth pred} \\
&= Pr(Class{i}) * IOU{truth pred}\\
\end{align}
$$

논문에서는 YOLO의 성능평가를 위해 PASCAL VOC을 사용하였으며, S, B, C에는 각각 7, 2, 20이 할당되었다. 

#### Network Design
YOLO의 network architecture는 GoogLeNet for image classification 모델을 기반으로 한다. 
(24 Convolutional layers & 2 Fully Connected layers - 참고로 Fast YOLO는 위 디자인의 24의 convolutional layer를 9개의 convolutional layer로 대체한다.)
![]({{ site.url }}/images/yolo/Figure3.JPG "Figure3"){: .aligncenter}
논문에 기재된 위 그림보다는 [Deep Systems](https://youtu.be/L0tzmv--CGY)의 슬라이드를 참고하는 것이 직관적으로 더 이해하기 쉬울 것 같다.
![]({{ site.url }}/images/yolo/DeepSystems-NetworkArchitecture.JPG "DeepSystems-NetworkArchitecture"){: .aligncenter}


#### Inference Process
참고슬라이드: [Deep System's YOLO - 15~70p](https://goo.gl/g9kYQT)    

![]({{ site.url }}/images/yolo/NetArch0.JPG "NetArch0"){: .aligncenter}
7X7은 49개의 Grid Cell을 의미한다. 그리고 각각의 Grid cell은 B개의 bounding Box를 가지고 있는데(여기선 B=2), 
앞 5개의 값은 해당 Grid cell의 첫 번째 bounding box에 대한 값이 채워져있다.

![]({{ site.url }}/images/yolo/NetArch1.JPG "NetArch1"){: .aligncenter}
6~10번째 값은 두 번째 bounding box에 대한 내용이다.

![]({{ site.url }}/images/yolo/NetArch2.JPG "NetArch2"){: .aligncenter}
나머지 20개의 값은 20개의 class에 대한 conditional class probability에 해당한다.

![]({{ site.url }}/images/yolo/NetArch3.JPG "NetArch3"){: .aligncenter}
첫 번째 bounding box의 confidence score와 각 conditional class probability를 곱하면 
첫 번째 bounding box의 class specific confidence score가 나온다.    
마찬가지로, 두 번째 bounding box의 confidence score와 각 conditional class probability를 곱하면 
두 번째 bounding box의 class specific confidence score가 나온다.

![]({{ site.url }}/images/yolo/NetArch4.JPG "NetArch4"){: .aligncenter}
이 계산을 각 bounding box에 대해 하게되면 총 98개의 class specific confidence score를 얻을 수 있다.

이 98개의 class specific confidence score에 대해 각 20개의 클래스를 기준으로 [non-maximum suppression](https://goo.gl/byNZTn)을 하여, Object에 대한 Class 및 bounding box Location를 결정한다.    
논문에는 따로 기술되지 않았지만 Error를 줄이기 위해 class specific confidence score에 대한 Threshold를 설정하지 않았을까 싶다.

#### Training Process
참고슬라이드: [YOLO CVPR 2016 - 30~43p](https://goo.gl/yXjGnv)

YOLO의 Loss function을 이해하는 것으로 Training process에 대한 설명을 대신할 수 있을 것 같다. 
Loss function을 뜯어보기 전에 전제조건 몇 가지를 먼저 보도록 하자.

1. Grid cell의 여러 bounding box들 중, ground-truth box와의 IOU가 가장높은 bounding box를 predictor로 설정한다.
2. 1의 기준에 따라 아래 기호들이 사용된다.

![]({{ site.url }}/images/yolo/notation.JPG "Notation"){: .aligncenter}

(1) Object가 존재하는 grid cell i의 predictor bounding box j    
(2) Object가 존재하지 않는 grid cell i의 bounding box j    
(3) Object가 존재하는 grid cell i    
**덧:** Ground-truth box의 중심점이 어떤 grid cell 내부에 위치하면, 그 grid cell에는 Object가 존재한다고 여긴다. 

**Loss Function:** 
![]({{ site.url }}/images/yolo/lossFunction.JPG "LossFunction"){: .aligncenter}

(1) Object가 존재하는 grid cell i의 predictor bounding box j에 대해, x와 y의 loss를 계산.    
(2) Object가 존재하는 grid cell i의 predictor bounding box j에 대해, w와 h의 loss를 계산. 큰 box에 대해서는 small deviation을 반영하기 위해 제곱근을 취한 후, sum-squared error를 한다.(같은 error라도 larger box의 경우 상대적으로 IOU에 영향을 적게 준다.)    
(3) Object가 존재하는 grid cell i의 predictor bounding box j에 대해, confidence score의 loss를 계산. ($$C_{i}$$ = 1)    
(4) Object가 존재하지 않는 grid cell i의 bounding box j에 대해, confidence score의 loss를 계산. ($$C_{i}$$ = 0)    
(5) Object가 존재하는 grid cell i에 대해, conditional class probability의 loss 계산. (Correct class c: $$p_{i}(c)$$=1, otherwise: $$p_{i}(c)$$=0)    

$$\lambda_{coord}$$: coordinates(x,y,w,h)에 대한 loss와 다른 loss들과의 균형을 위한 balancing parameter.    
$$\lambda_{noobj}$$: obj가 있는 box와 없는 box간에 균형을 위한 balancing parameter. (일반적으로 image내에는 obj가 있는 cell보다는 obj가 없는 cell이 훨씬 많으므로) 

#### Training YOLO

- ImageNet 1000-class dataset으로 20개의 convolutioanl layer를 pre-training
- Pre-training 이후 4 convolutioanl layers와 2 fully connected layers를 추가
- Bounding Box의 width와 height는 이미지의 width와 height로 nomalize (Range: 0~1)
- Bounding Box의 x와 y는 특정 grid cell 위치의 offset값을 사용한다 (Range: 0~1)
- $$\lambda_{coord}$$: 5, $$\lambda_{noobj}$$: 0.5
- Batch size: 64
- Momentum: 0.9 and a decay of 0.0005
- Learning Rate: 0.001에서 0.01로 epoch마다 천천히 상승시킴. 이후 75 epoch동안 0.01, 30 epoch동안 0.001, 마지막 30 epoch동안 0.0001
- Dropout Rate: 0.5
- Data augmentation: random scailing and translations of up to 20% of the original image size
- Activation function: leaky rectified linear activation
![]({{ site.url }}/images/yolo/leakyRelu.JPG "LeakyRelu"){: .aligncenter}

#### Limitation of YOLO

1. 각각의 grid cell이 하나의 클래스만을 예측할 수 있으므로, 작은 object 여러개가 다닥다닥 붙으면 제대로 예측하지 못한다. 
2. bounding box의 형태가 training data를 통해서만 학습되므로, 새로운/독특한 형태의 bouding box의 경우 정확히 예측하지 못한다.
3. 몇 단계의 layer를 거쳐서 나온 feature map을 대상으로 bouding box를 예측하므로 localization이 다소 부정확해지는 경우가 있다.

# Experiments

![]({{ site.url }}/images/yolo/Table1.JPG "Table1"){: .aligncenter}
Table1: 다른 real-time object detect system들에 비해 높은 mAP를 보여준다. Fast YOLO의 경우 가장 빠른 속도를 보여준다.

![]({{ site.url }}/images/yolo/Figure4.JPG "Figure4"){: .aligncenter}
Figure4: Fast R-CNN과 비교했을 떄, 훨씬 적은 False-Positive를 보여준다. (low backgound error)

![]({{ site.url }}/images/yolo/Table2.JPG "Table2"){: .aligncenter}
Table2: Fast R-CNN과 같이 동작하면 Fast R-CNN을 보완하는 역할을 할 수 있다. (low backgound error)

![]({{ site.url }}/images/yolo/Table3.JPG "Table3"){: .aligncenter}

![]({{ site.url }}/images/yolo/FIgure5.JPG "Figure5"){: .aligncenter}
Figure5: Natural image training -> Artwork detection 에서 매우 강한 면모를 보인다.

# Conclusion
![]({{ site.url }}/images/yolo/Figure6.JPG "Figure6"){: .aligncenter}
간단하다. 빠르다. 그리고 빠르다!

# References

* Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi, "You Only Look Once: Unified, Real-Time Object Detection", [https://arxiv.org/abs/1506.02640](https://arxiv.org/abs/1506.02640)
* Joseph Chet Redmon's blog, "YOLO CVPR 2016", [https://goo.gl/Xj2Eik](https://goo.gl/Xj2Eik)
* Deep Systems, "YOLO: You only look once (How it works)", [https://youtu.be/L0tzmv--CGY](https://youtu.be/L0tzmv--CGY)
* Darknet, "Implementaion", [https://github.com/pjreddie/darknet](https://github.com/pjreddie/darknet)

**Special thanks to JHS & PJB**
